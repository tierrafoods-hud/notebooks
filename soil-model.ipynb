{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file defines and functions\n",
    "def makeOutputDir(directory, baseDir='./outputs'):\n",
    "    date = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "    output_dir = f'{baseDir}/{date}/{directory}'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "def check_missing_values(dataset):\n",
    "    return dataset.isnull().sum().to_frame()\n",
    "\n",
    "def replace_invalid_dates(date_str):\n",
    "    # Split the date string into components\n",
    "    parts = date_str.split('-')\n",
    "    \n",
    "    # Check if the year is valid\n",
    "    if len(parts) < 1 or not parts[0].isdigit() or len(parts[0]) != 4:\n",
    "        return pd.NaT  # Return Not a Time if year is invalid\n",
    "    \n",
    "    year = parts[0]\n",
    "    \n",
    "    # Check if the month is valid\n",
    "    if len(parts) < 2 or not parts[1].isdigit() or not (1 <= int(parts[1]) <= 12):\n",
    "        month = '01'  # Default to January if month is invalid\n",
    "    else:\n",
    "        month = parts[1].zfill(2)  # Ensure month is two digits\n",
    "    \n",
    "    # Check if the day is valid\n",
    "    if len(parts) < 3 or not parts[2].isdigit() or not (1 <= int(parts[2]) <= 31):\n",
    "        day = '01'  # Default to the first day of the month if day is invalid\n",
    "    else:\n",
    "        day = parts[2].zfill(2)  # Ensure day is two digits\n",
    "    \n",
    "    # Construct the valid date string\n",
    "    valid_date_str = f\"{year}-{month}-{day}\"\n",
    "    \n",
    "    # Convert to datetime\n",
    "    return pd.to_datetime(valid_date_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# constants\n",
    "plot_title = \"Country Mexico\"\n",
    "country_name = \"Mexico\"\n",
    "\n",
    "dataset_file_name = \"Mexico_wosis_merged.csv\"\n",
    "master_dataset_file_path = f\"../data/soil/{dataset_file_name}\"\n",
    "\n",
    "output_dir = makeOutputDir(country_name, \"../outputs\")\n",
    "saveModelPath = makeOutputDir(country_name, \"../models\")\n",
    "\n",
    "drop_columns = ['date', 'latitude', 'longitude', 'country_name', 'region', 'continent']\n",
    "\n",
    "# model list\n",
    "models = {\n",
    "    'Random Forest Regressor': RandomForestRegressor(),\n",
    "    'Gradient Boosting Regressor': GradientBoostingRegressor(),\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Support Vector Regressor': SVR(),\n",
    "    'K-Nearest Neighbors Regressor': KNeighborsRegressor()\n",
    "}\n",
    "\n",
    "# -- do not change below this line --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "\n",
    "Load master dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load master dataset\n",
    "master_dataset = pd.read_csv(master_dataset_file_path)\n",
    "\n",
    "# replace invalid dates\n",
    "master_dataset['date'] = master_dataset['date'].apply(replace_invalid_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values\n",
    "missing_values = check_missing_values(master_dataset)\n",
    "sns.heatmap(missing_values, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title(f\"Missing valus in {plot_title} dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove missing values\n",
    "\n",
    "- Removing the columns with missing values which are more than 90% of length of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing columns with threshold\n",
    "master_dataset = master_dataset.dropna(thresh=0.3 * len(master_dataset), axis=1)\n",
    "print(f\"master_dataset columns: {master_dataset.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Create a  new column for Bulk Density, Organic Matter based on formulaes below\n",
    "\n",
    "Create a new column for combination of silt_plus_clay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create organic matter\n",
    "master_dataset['organic_matter'] = master_dataset.apply(lambda row: 1.724 * row['orgc'], axis=1)\n",
    "\n",
    "# create bulk density\n",
    "master_dataset[f'bulk_density'] = master_dataset.apply(lambda row: 1.62-0.06 * row['organic_matter'], axis=1)\n",
    "\n",
    "# create sum of silt plus clay\n",
    "master_dataset[f'silt_plus_clay'] = master_dataset.apply(lambda row: (row['silt'] if not pd.isnull(row['silt']) else 0) + (row['clay'] if not pd.isnull(row['clay']) else 0) , axis=1)\n",
    "\n",
    "# drop silt and clay column\n",
    "master_dataset = master_dataset.drop(columns=['silt', 'clay']).copy()\n",
    "\n",
    "print(f\"\\n Length of dataset: {len(master_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Negative Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove negative values ignore latitude and longitude\n",
    "numeric_columns_dataset = master_dataset.select_dtypes(include=['int64', 'float64']).columns.difference(['latitude', 'longitude'])\n",
    "# master_dataset = master_dataset[master_dataset[numeric_columns_dataset] >= 0]\n",
    "# master_dataset = master_dataset[master_dataset[numeric_columns_dataset] >= 0]\n",
    "\n",
    "print(\"Length of numeric columns:\", len(numeric_columns_dataset))\n",
    "print(numeric_columns_dataset)\n",
    "master_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising data distribution\n",
    "import math\n",
    "def plot_distribution_charts(numeric_columns, dataset, title=\"\"):\n",
    "    num_cols = len(numeric_columns)\n",
    "\n",
    "    # Define the number of rows and columns dynamically\n",
    "    chart_cols = 3  # Fixed number of columns\n",
    "    chart_rows = math.ceil(num_cols / chart_cols)  # Calculate required rows based on columns\n",
    "\n",
    "    fig, axes = plt.subplots(chart_rows, chart_cols, figsize=(15, 3 * chart_rows), constrained_layout=True)\n",
    "    axes = axes.flatten()  # Flatten the axes for easy iteration\n",
    "\n",
    "    for i, col in enumerate(numeric_columns):\n",
    "        sns.histplot(dataset[col], kde=True, ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Column distribution {title}\")\n",
    "    plt.show()\n",
    "\n",
    "plot_distribution_charts(numeric_columns_dataset, master_dataset, plot_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Outliers using Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers for orgc and tceq\n",
    "for col in numeric_columns_dataset:\n",
    "    # setting upper and lower limit\n",
    "    upper_limit = master_dataset[col].mean() + 3*master_dataset[col].std()\n",
    "    lower_limit = master_dataset[col].mean() - 3*master_dataset[col].std()\n",
    "\n",
    "    # count outliers\n",
    "    count_outliers = master_dataset[(master_dataset[col] > upper_limit) | (master_dataset[col] < lower_limit)]\n",
    "\n",
    "    # trim outlier using z-score\n",
    "    master_dataset = master_dataset[(master_dataset[col] >= lower_limit) & (master_dataset[col] <= upper_limit)]\n",
    "\n",
    "    if(len(count_outliers) > 0):\n",
    "        print(f\"Processing column {col}\")\n",
    "        print(\"Upper limit\",upper_limit, \"Lower limit\", lower_limit)\n",
    "        print(\"Total Outliers\", len(count_outliers))\n",
    "\n",
    "print(f\"\\n Length of dataset: {len(master_dataset)}\")\n",
    "master_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_charts(numeric_columns_dataset, master_dataset, \"after removing outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing Missing Values with avg of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with mean\n",
    "master_dataset[numeric_columns_dataset] = master_dataset[numeric_columns_dataset].apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "print(f\"\\n Length of dataset: {len(master_dataset)}\")\n",
    "master_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Length & Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into orgc and tceq\n",
    "orgc_df = master_dataset.drop(columns=['tceq']).copy()\n",
    "tceq_df = master_dataset.drop(columns=['orgc']).copy()\n",
    "\n",
    "# count data & missing values\n",
    "print(f\"orgc_df length: {len(orgc_df)}\")\n",
    "print(f\"tceq_df length: {len(tceq_df)}\")\n",
    "\n",
    "# plot missing values\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\n",
    "orgc_missing_values = check_missing_values(orgc_df)\n",
    "sns.heatmap(orgc_missing_values, annot=True, fmt='g', cmap='Blues', ax=ax1)\n",
    "ax1.set_title(f\"Missing valus in ORGC for {plot_title}\")\n",
    "\n",
    "tceq_missing_values = check_missing_values(tceq_df)\n",
    "sns.heatmap(tceq_missing_values, annot=True, fmt='g', cmap='Blues', ax=ax2)\n",
    "ax2.set_title(f\"Missing valus in TCEQ for {plot_title}\")\n",
    "plt.suptitle(f\"Missing values after splitting dataset {plot_title}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets\n",
    "orgc_df.to_csv(f\"{output_dir}/{country_name}_wosis_orgc.csv\", index=False)\n",
    "tceq_df.to_csv(f\"{output_dir}/{country_name}_wosis_tceq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot line chart of orgc and tceq for each year\n",
    "orgc_df['year'] = orgc_df['date'].dt.year\n",
    "tceq_df['year'] = tceq_df['date'].dt.year\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 8))\n",
    "\n",
    "sns.lineplot(data=orgc_df.groupby('year')['orgc'].count().reset_index(), x='year', y='orgc', ax=ax[0, 0])\n",
    "ax[0, 0].set_title(f'Count of ORGC by Year {plot_title}')\n",
    "ax[0, 0].set_xlabel('Year')\n",
    "ax[0, 0].set_ylabel('Count of ORGC')\n",
    "\n",
    "sns.lineplot(data=tceq_df.groupby('year')['tceq'].count().reset_index(), x='year', y='tceq', ax=ax[0, 1])\n",
    "ax[0, 1].set_title(f'Count of TCEQ by Year {plot_title}')\n",
    "ax[0, 1].set_xlabel('Year')\n",
    "ax[0, 1].set_ylabel('Count of TCEQ')\n",
    "\n",
    "sns.lineplot(data=orgc_df.groupby('year')['orgc'].mean().reset_index(), x='year', y='orgc', ax=ax[1, 0])\n",
    "ax[1, 0].set_title(f'Average ORGC by Year {plot_title}')\n",
    "ax[1, 0].set_xlabel('Year')\n",
    "ax[1, 0].set_ylabel('Average ORGC')\n",
    "\n",
    "sns.lineplot(data=tceq_df.groupby('year')['tceq'].mean().reset_index(), x='year', y='tceq', ax=ax[1, 1])\n",
    "ax[1, 1].set_title(f'Average TCEQ by Year {plot_title}')\n",
    "ax[1, 1].set_xlabel('Year')\n",
    "ax[1, 1].set_ylabel('Average TCEQ')\n",
    "\n",
    "plt.suptitle(f'Line Charts of ORGC and TCEQ by Year {plot_title}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers\n",
    "def plot_outliers(dataframes, title):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "    for i, dataframe in enumerate(dataframes):\n",
    "        sns.boxplot(data=dataframe, ax=axes[i])\n",
    "        axes[i].set_title(f'{title} {plot_title}')\n",
    "        axes[i].set_xlabel('Columns')\n",
    "        axes[i].set_ylabel('Values')\n",
    "        axes[i].set_xticks(ticks=range(len(dataframe.columns)), labels=dataframe.columns, rotation=45)\n",
    "\n",
    "    plt.suptitle(f'Outliers in ORGC and TCEQ datasets {plot_title}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create copy of year column\n",
    "orgc_year = orgc_df['year'].copy()\n",
    "tceq_year = tceq_df['year'].copy()\n",
    "\n",
    "# append to drop colum\n",
    "drop_columns.append(\"year\")\n",
    "\n",
    "plot_outliers([orgc_df.drop(columns=drop_columns), tceq_df.drop(columns=drop_columns)], 'ORGC and TCEQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data using standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "orgc_scaled = scaler.fit_transform(orgc_df.drop(columns=drop_columns))\n",
    "standard_orgc_df = pd.DataFrame(orgc_scaled, columns=orgc_df.drop(columns=drop_columns).columns)\n",
    "\n",
    "tceq_scaled = scaler.fit_transform(tceq_df.drop(columns=drop_columns))\n",
    "standard_tceq_df = pd.DataFrame(tceq_scaled, columns=tceq_df.drop(columns=drop_columns).columns)\n",
    "\n",
    "plot_outliers([standard_orgc_df, standard_tceq_df], 'ORGC and TCEQ (after standard scaling)')\n",
    "\n",
    "standard_orgc_df['year'] = orgc_year\n",
    "standard_tceq_df['year'] = tceq_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data using min max scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "orgc_scaled_minmax = scaler.fit_transform(orgc_df.drop(columns=drop_columns))\n",
    "minmax_orgc_df = pd.DataFrame(orgc_scaled_minmax, columns=orgc_df.drop(columns=drop_columns).columns)\n",
    "\n",
    "tceq_scaled_minmax = scaler.fit_transform(tceq_df.drop(columns=drop_columns))\n",
    "minmax_tceq_df = pd.DataFrame(tceq_scaled_minmax, columns=tceq_df.drop(columns=drop_columns).columns)\n",
    "\n",
    "plot_outliers([minmax_orgc_df, minmax_tceq_df], 'ORGC and TCEQ (after min max scaling)')\n",
    "\n",
    "minmax_orgc_df['year'] = orgc_year\n",
    "minmax_tceq_df['year'] = tceq_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confussion Matrix\n",
    "Illustrate the relationship between features using confussion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "corr_matrix = standard_orgc_df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=axes[0])\n",
    "axes[0].set_title(f'ORGC {country_name} (after standard scaling)')\n",
    "\n",
    "corr_matrix = standard_tceq_df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=axes[1])\n",
    "axes[1].set_title(f'TCEQ {country_name} (after standard scaling)')\n",
    "\n",
    "plt.suptitle('Correlation Matrix of ORGC and TCEQ datasets')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "\n",
    "def train_model(X, y, model):\n",
    "    # split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate regression metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return mse, mae, r2, y_test, y_pred\n",
    "\n",
    "def validate_model(models, features, target, title, save_model=False):\n",
    "    best_model = None\n",
    "    best_model_score = float('inf')  # Initialize best model score as infinity\n",
    "\n",
    "    # train the models\n",
    "    fig, axes = plt.subplots(1, len(models), figsize=(20, 5))\n",
    "    print(title)\n",
    "    for i, (model_name, model) in enumerate(models.items()):\n",
    "        mse, mae, r2, y_test, y_pred = train_model(features, target, model)\n",
    "\n",
    "        print(f'{model_name} - MSE: {mse}, MAE: {mae}, R2: {r2}')\n",
    "        # joblib.dump(model, f'{saveModelPath}{title}_{model_name}.pkl')\n",
    "\n",
    "        # Scatter plot of predicted vs actual\n",
    "        axes[i].scatter(y_test, y_pred, alpha=0.7, edgecolors='k')\n",
    "        axes[i].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "        axes[i].set_title(f'{model_name}\\n(MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f})', fontsize=10)\n",
    "        axes[i].set_xlabel('Actual')\n",
    "        axes[i].set_ylabel('Predicted')\n",
    "\n",
    "        # Check if this model is better than the current best model\n",
    "        if mae < best_model_score:\n",
    "            best_model = model\n",
    "            best_model_score = mae\n",
    "\n",
    "    # Save the best model if specified\n",
    "    if save_model and best_model:\n",
    "        joblib.dump(best_model, f'{saveModelPath}/{title}_best_model.pkl')\n",
    "        print(\"Best model saved in: \", f'{saveModelPath}/{title}_best_model.pkl')\n",
    "\n",
    "    plt.suptitle(f'Scatter plots of {title} models')\n",
    "    plt.tight_layout()\n",
    "    plt.figtext(0.5, 0.005,\n",
    "                'The scatter plots show the predicted vs actual values for each model. '\n",
    "                'The red dashed line represents the perfect prediction.', \n",
    "                ha='center', fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning model on data from year 2000 onward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaled ORGC\n",
    "orgc_year_2000 = standard_orgc_df[standard_orgc_df['year'] >= 2000].copy()\n",
    "orgc_year_2000 = orgc_year_2000.drop(columns=['year'])\n",
    "validate_model(models, orgc_year_2000.drop(columns=['orgc']), orgc_year_2000['orgc'], f'{country_name}_ORGC', save_model=True)\n",
    "\n",
    "# Standard Scaled TCEQ\n",
    "tceq_year_2000 = standard_tceq_df[standard_tceq_df['year'] >= 2000].copy()\n",
    "tceq_year_2000 = tceq_year_2000.drop(columns=['year'])\n",
    "validate_model(models, tceq_year_2000.drop(columns=['tceq']), tceq_year_2000['tceq'], f'{country_name}_TCEQ', save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Transferability to past data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_transferability(standard_df_dict, model_dict, saveModelPath, country_name):\n",
    "    \"\"\"\n",
    "    Plot model transferability to historical data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    standard_df_dict : dict\n",
    "        Dictionary containing standardized dataframes for each target variable\n",
    "        e.g. {'orgc': standard_orgc_df, 'tceq': standard_tceq_df}\n",
    "    model_dict : dict \n",
    "        Dictionary containing trained models for each target variable\n",
    "        e.g. {'orgc': orgc_model, 'tceq': tceq_model}\n",
    "    saveModelPath : str\n",
    "        Path where models are saved\n",
    "    country_name : str\n",
    "        Name of the country for which models were trained\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None. Displays plot comparing model performance on historical data.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(model_dict), figsize=(15, 4))\n",
    "    \n",
    "    for i, (target_var, model) in enumerate(model_dict.items()):\n",
    "        # Get data for this target\n",
    "        df = standard_df_dict[target_var]\n",
    "        historical_data = df[df['year'] < 2000].copy()\n",
    "        \n",
    "        # Prepare features and target\n",
    "        features = historical_data.drop(columns=['year', target_var])\n",
    "        target = historical_data[target_var]\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(features)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(target, predictions)\n",
    "        mae = mean_absolute_error(target, predictions)\n",
    "        r2 = r2_score(target, predictions)\n",
    "        \n",
    "        # Create scatter plot\n",
    "        axes[i].scatter(target, predictions, alpha=0.7, edgecolors='k')\n",
    "        axes[i].plot([target.min(), target.max()], [target.min(), target.max()], 'r--')\n",
    "        axes[i].set_title(f'{target_var.upper()} Model Performance on Historical Data\\n'\n",
    "                         f'(MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f})')\n",
    "        axes[i].set_xlabel(f'Actual {target_var.upper()}')\n",
    "        axes[i].set_ylabel(f'Predicted {target_var.upper()}')\n",
    "\n",
    "    plt.suptitle('Model Transferability to Historical Data (Pre-2000)')\n",
    "    plt.tight_layout()\n",
    "    plt.figtext(0.5, 0.01,\n",
    "                'The scatter plots show the predicted vs actual values for historical data. '\n",
    "                'The red dashed line represents the perfect prediction.',\n",
    "                ha='center', fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "standard_df_dict = {\n",
    "    'orgc': standard_orgc_df,\n",
    "    'tceq': standard_tceq_df\n",
    "}\n",
    "\n",
    "model_dict = {\n",
    "    'orgc': joblib.load(f'{saveModelPath}/{country_name}_ORGC_best_model.pkl'),\n",
    "    'tceq': joblib.load(f'{saveModelPath}/{country_name}_TCEQ_best_model.pkl')\n",
    "}\n",
    "\n",
    "plot_model_transferability(standard_df_dict, model_dict, saveModelPath, country_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model Training & Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_scale(featurs, target, test_size=0.2, random_state=42):\n",
    "    # split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(featurs, target, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # scaling for models like SVR and KNN\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_train_scaled, X_test, X_test_scaled, y_train, y_test\n",
    "\n",
    "def build_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),  # Use Input to specify the input shape\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def validate_dl_model(features, target, title, random_state=42, validation_split=0.2, epochs=100, batch_size=32, verbose=0):\n",
    "    X_train, X_train_scaled, X_test, X_test_scaled, y_train, y_test = split_and_scale(features, target, test_size=validation_split, random_state=random_state)\n",
    "\n",
    "    # initialise and train\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    model = build_model(input_dim)\n",
    "    history = model.fit(X_train_scaled, y_train, validation_split=validation_split, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    y_pred = model.predict(X_test_scaled).flatten()\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Deep Learning Model - MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    # Check for overfitting/underfitting\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    if val_loss > train_loss * 1.1:  # 10% threshold\n",
    "        print(\"Model shows signs of overfitting (validation loss notably higher than training loss)\")\n",
    "    elif val_loss < train_loss * 0.9:  # 10% threshold\n",
    "        print(\"Model shows signs of underfitting (validation loss notably lower than training loss)\")\n",
    "    else:\n",
    "        print(\"Model shows good fit (validation and training losses are close)\")\n",
    "\n",
    "    # Plot training history\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f\"Deep Learning Model {title} \\nSplit: {validation_split} \\nMSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = numeric_columns_dataset.difference(['tceq', 'orgc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "various_splits = [0.2, 0.3, 0.4]\n",
    "\n",
    "for split_num in various_splits:\n",
    "    # deep learning Orgc\n",
    "    validate_dl_model(orgc_df[num_cols], orgc_df['orgc'], title=\"ORGC\", validation_split=split_num)\n",
    "\n",
    "    # deep learning Tceq\n",
    "    validate_dl_model(tceq_df[num_cols], tceq_df['tceq'], title=\"TCEQ\", validation_split=split_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Feature Engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
