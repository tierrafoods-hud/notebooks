{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load raw dataset\n",
    "dataset = pd.read_csv(\"D:/tierra/data/mexico_combined_data.csv\")\n",
    "lat_long_cols = ['latitude', 'longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle pre-processing\n",
    "def get_numeric_cols(dataset):\n",
    "    # numeric columns except for latitude and longitude\n",
    "    numeric_cols = dataset.select_dtypes(include=['number']).columns\n",
    "    numeric_cols = numeric_cols.drop(lat_long_cols)\n",
    "    return numeric_cols\n",
    "\n",
    "numeric_cols = get_numeric_cols(dataset)\n",
    "# Calculate percentage of missing values for each column\n",
    "missing_pct = dataset[numeric_cols].isnull().mean()\n",
    "\n",
    "# plot missing values\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# missing_pct.sort_values(ascending=True).plot(kind='bar')\n",
    "# plt.title('Percentage of Missing Values by Column')\n",
    "# plt.xlabel('Columns')\n",
    "# plt.ylabel('Percentage Missing')\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date to datetime\n",
    "dataset['date'] = pd.to_datetime(dataset['date'].values, format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "# drop columns with more than 70% missing values\n",
    "dataset = dataset.dropna(thresh=0.3 * len(dataset), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from geopy.distance import geodesic\n",
    "import numpy as np\n",
    "\n",
    "def fill_missing_values_by_distance(df, feature_col, lat=\"latitude\", long=\"longitude\"):\n",
    "    # Create a copy to avoid modifying original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Get missing and valid data\n",
    "    missing_mask = df_copy[feature_col].isnull()\n",
    "    \n",
    "    # If no missing values, return original\n",
    "    if not missing_mask.any():\n",
    "        return df_copy[feature_col]\n",
    "        \n",
    "    missing_coords = df_copy.loc[missing_mask, [lat, long]].values\n",
    "    valid_coords = df_copy.loc[~missing_mask, [lat, long]].values\n",
    "    valid_values = df_copy.loc[~missing_mask, feature_col].values\n",
    "    \n",
    "    # Calculate distances one at a time to handle single points\n",
    "    distances = []\n",
    "    for m in missing_coords:\n",
    "        dist_row = []\n",
    "        for v in valid_coords:\n",
    "            dist_row.append(geodesic(m, v).km)\n",
    "        distances.append(dist_row)\n",
    "    distances = np.array(distances)\n",
    "    \n",
    "    # Handle case of single missing point\n",
    "    if len(distances.shape) == 1:\n",
    "        nearest_index = distances.argmin()\n",
    "    else:\n",
    "        nearest_indices = distances.argmin(axis=1)\n",
    "        df_copy.loc[missing_mask, feature_col] = valid_values[nearest_indices]\n",
    "    \n",
    "    return df_copy[feature_col]\n",
    "\n",
    "# Fill missing values for all numeric columns at once\n",
    "numeric_cols = get_numeric_cols(dataset)\n",
    "# dataset[numeric_cols] = dataset[numeric_cols].apply(\n",
    "#     lambda col: fill_missing_values_by_distance(dataset, col.name)\n",
    "# )\n",
    "\n",
    "# fill missing values with the mean of the column\n",
    "dataset[numeric_cols] = dataset[numeric_cols].fillna(dataset[numeric_cols].mean())\n",
    "\n",
    "df = dataset[numeric_cols.tolist() + lat_long_cols].copy()\n",
    "\n",
    "# plot distribution of numeric columns\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(df['orgc'], bins=30, kde=True)\n",
    "# plt.title(f'Distribution of orgc')\n",
    "# plt.show()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate organic matter metrics if orgc present\n",
    "if 'orgc' in df.columns:\n",
    "    # Use vectorized operations for better performance\n",
    "    organic_matter = 1.724 * df['orgc']\n",
    "    df = df.assign(\n",
    "        organic_matter=organic_matter,\n",
    "        bulk_density=1.62 - 0.06 * organic_matter\n",
    "    )\n",
    "\n",
    "# Handle silt and clay columns if present\n",
    "if {'silt', 'clay'}.issubset(df.columns):\n",
    "    df['silt_plus_clay'] = df[['silt', 'clay']].sum(axis=1, skipna=True)\n",
    "    df.drop(columns=['silt', 'clay'], inplace=True)\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import folium\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# introduce geospatial features\n",
    "def load_grids(file_path):\n",
    "    grid_gdf = gpd.read_file(file_path)\n",
    "    return grid_gdf\n",
    "\n",
    "def load_folium_map(grid_gdf):\n",
    "    center_lat = grid_gdf.geometry.centroid.y.mean()\n",
    "    center_lon = grid_gdf.geometry.centroid.x.mean()\n",
    "\n",
    "    # # create a folium map\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=8)\n",
    "\n",
    "    folium.GeoJson(\n",
    "            grid_gdf, \n",
    "            name=\"10km Grid\", \n",
    "            style_function=lambda x: {'color': 'blue', 'weight': 0.2, 'fillOpacity': 0},\n",
    "            highlight_function=lambda x: {'weight': 3, 'fillOpacity': 0.5},\n",
    "    ).add_to(m)\n",
    "\n",
    "    # show the map\n",
    "    return m\n",
    "\n",
    "grid_gdf = load_grids(\"D:/tierra/data/grids/mexico_grid_10km.shp\")\n",
    "# Create GeoDataFrame with geometry and sequential grid_id\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    {'grid_id': range(1, len(grid_gdf) + 1)}, \n",
    "    geometry=grid_gdf.geometry\n",
    ")\n",
    "\n",
    "# Convert df to GeoDataFrame using latitude/longitude\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "\n",
    "# Perform spatial join between points and grid polygons\n",
    "gdf = gpd.sjoin(gdf, df_gdf, how=\"left\", predicate=\"contains\")\n",
    "\n",
    "# Drop unnecessary columns from the spatial join\n",
    "gdf = gdf.drop(columns=['index_right'], errors='ignore')\n",
    "\n",
    "print(gdf.shape)\n",
    "print(gdf.columns)\n",
    "gdf.head()\n",
    "\n",
    "# create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "# plot frequency of grid_id\n",
    "ax1.hist(gdf['grid_id'], bins=100)\n",
    "ax1.set_title('Frequency of Grid ID')\n",
    "ax1.set_xlabel('Grid ID')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "# plot heatmap of grid_id\n",
    "hist2d = ax2.hist2d(gdf.geometry.centroid.x, gdf.geometry.centroid.y, bins=50, cmap='YlOrRd')\n",
    "ax2.set_title('Spatial Distribution of Sample Points')\n",
    "ax2.set_xlabel('Longitude')\n",
    "ax2.set_ylabel('Latitude')\n",
    "\n",
    "# adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce spatial features in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Input # type: ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# deep learning for organic carbon prediction\n",
    "X = df.drop(columns=['orgc'])\n",
    "y = df['orgc']\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# build the model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'), # first hidden layer with 64 neurons\n",
    "    Dense(32, activation='relu'), # second hidden layer with 32 neurons\n",
    "    Dense(1) # output layer with 1 neuron\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# plot the training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Training History Organic Carbon')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# plot the predictions vs the actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r:', label='Perfect Prediction')\n",
    "plt.xlabel('Actual Organic Carbon (g/Kg)')\n",
    "plt.ylabel('Predicted Organic Carbon (g/Kg)')\n",
    "plt.title('Actual vs Predicted Organic Carbon')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
