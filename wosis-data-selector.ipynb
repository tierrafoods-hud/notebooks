{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuration settings for processing WoSIS (World Soil Information Service) data\n",
    "\n",
    "This configuration file contains settings for:\n",
    "- Input/output file paths and formats\n",
    "- Required soil property layers to extract\n",
    "- Geographic and depth filtering parameters \n",
    "- Data type specifications for memory optimization\n",
    "\n",
    "The WoSIS dataset contains standardized soil profile data from various sources worldwide.\n",
    "For more details see: https://www.isric.org/explore/wosis\n",
    "\"\"\"\n",
    "\n",
    "# Input/Output Configuration\n",
    "input_file = \"../data/WoSIS_2023_December/wosis_202312.gpkg\"  # GeoPackage containing WoSIS data\n",
    "layer_prefix = \"wosis_202312_\"  # Prefix for layer names in the GeoPackage\n",
    "output_path = \"../outputs/\"  # Directory for processed outputs\n",
    "\n",
    "# Required Data Layers\n",
    "# List of soil properties to extract (see WoSIS documentation for descriptions)\n",
    "required_layers = [\"bdfiad\", \"bdfiod\", \"bdwsod\", \"cecph7\", \"cecph8\", \"cfvo\", \"clay\", \"ecec\", \"elco50\", \"nitkjd\", \"orgc\", \n",
    "    \"orgm\", \"phaq\", \"phetm3\", \"sand\", \"silt\", \"tceq\", \"totc\", \"wv0010\", \"wv0033\", \"wv1500\"]\n",
    "\n",
    "# Required columns to keep from each layer\n",
    "required_columns = [\n",
    "    'date', 'longitude', 'latitude',\n",
    "    'upper_depth', 'lower_depth',\n",
    "    'country_name', 'region', 'continent',\n",
    "    'value_avg'\n",
    "]\n",
    "\n",
    "# Geographic Filtering\n",
    "country_name = \"Mexico\"  # Country to filter data for\n",
    "# Bounding box for Mexico [min_lon, min_lat, max_lon, max_lat]\n",
    "bounding_box = None #[-117.12776, 14.5388286402, -86.811982388, 32.72083]\n",
    "\n",
    "# duration\n",
    "start_date = \"1900-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "\n",
    "# Depth Filtering (in cm)\n",
    "max_upper_depth = 30  # Maximum depth for upper boundary\n",
    "max_lower_depth = 30  # Maximum depth for lower boundary\n",
    "min_upper_depth = 0   # Minimum depth for upper boundary\n",
    "min_lower_depth = 0   # Minimum depth for lower boundary\n",
    "\n",
    "# Data Type Specifications for Memory Optimization\n",
    "dtype_dict = {\n",
    "    'date': 'str',\n",
    "    'longitude': 'float32',\n",
    "    'latitude': 'float32',\n",
    "    'country_name': 'category',\n",
    "    'region': 'category',\n",
    "    'continent': 'category',\n",
    "    'upper_depth': 'float32',\n",
    "    'lower_depth': 'float32',\n",
    "    'value_avg': 'float32'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "import logging\n",
    "import random\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def validate_config(\n",
    "    input_file: str,\n",
    "    layer_prefix: str, \n",
    "    required_layers: List[str],\n",
    "    required_columns: List[str],\n",
    "    output_path: str,\n",
    "    country_name: str,\n",
    "    bounding_box: Optional[List[float]]\n",
    ") -> Tuple[str, str, Optional[List[float]]]:\n",
    "    \"\"\"\n",
    "    Validate configuration parameters and create output directory if needed.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of validated country_name, output_path, and bounding_box\n",
    "    \"\"\"\n",
    "    # Validate required parameters\n",
    "    if not all([input_file, layer_prefix, required_layers, required_columns, output_path]):\n",
    "        raise ValueError(\"All configuration parameters must be provided\")\n",
    "\n",
    "    # Set defaults\n",
    "    country_name = country_name or \"Mexico\"\n",
    "    \n",
    "    # Validate bounding box if provided\n",
    "    if bounding_box and len(bounding_box) != 4:\n",
    "        raise ValueError(\"Bounding box must contain exactly 4 values: min_lat, max_lat, min_lon, max_lon\")\n",
    "        \n",
    "    # Check input file exists\n",
    "    if not os.path.exists(input_file) and not input_file.startswith(('http://', 'https://')):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "\n",
    "    # Create output directory\n",
    "    if not os.path.exists(output_path):\n",
    "        output_path = os.path.join(os.path.dirname(input_file), country_name)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "    return country_name, output_path, bounding_box\n",
    "\n",
    "def filter_by_country(df: pd.DataFrame, country_name: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Filter dataframe by country name\"\"\"\n",
    "    return df[df['country_name'] == country_name] if country_name in df['country_name'].unique() else None\n",
    "\n",
    "def process_layer(\n",
    "    input_file: str,\n",
    "    layer: str,\n",
    "    layer_prefix: str,\n",
    "    country_name: str,\n",
    "    required_columns: List[str],\n",
    "    bounding_box: Optional[List[float]],\n",
    "    depth_filters: Dict[str, float],\n",
    "    observations: pd.DataFrame,\n",
    "    duration: Optional[Dict[str, str]]\n",
    ") -> Optional[gpd.GeoDataFrame]:\n",
    "    \"\"\"Process a single layer with all filtering steps\"\"\"\n",
    "    \n",
    "    # Read layer data\n",
    "    df = gpd.read_file(input_file, layer=f\"{layer_prefix}{layer}\")\n",
    "\n",
    "    # Get observation description\n",
    "    obs_str = f\"{layer.upper()} - {observations[observations['code'] == layer.upper()]['property'].iloc[0]}\" \\\n",
    "        if layer.upper() in observations['code'].unique() else f\"Invalid {layer.upper()}\"\n",
    "    \n",
    "    # Apply filters\n",
    "    df = filter_by_country(df, country_name)\n",
    "    if df is None or df.empty:\n",
    "        logger.warning(f\"No data found for {obs_str}\")\n",
    "        return None\n",
    "    \n",
    "    # Apply bounding box filter if specified\n",
    "    if bounding_box:\n",
    "        df = df[df['latitude'].between(bounding_box[1], bounding_box[3]) & df['longitude'].between(bounding_box[0], bounding_box[2])]\n",
    "    \n",
    "    # Select and rename columns\n",
    "    df = df[required_columns].copy()\n",
    "    df = df.rename(columns={'value_avg': f'{layer}'})\n",
    "    \n",
    "    # Apply depth filters\n",
    "    depth_mask = (\n",
    "        (df['upper_depth'] <= depth_filters['max_upper']) & \n",
    "        (df['upper_depth'] >= depth_filters['min_upper']) &\n",
    "        (df['lower_depth'] <= depth_filters['max_lower']) & \n",
    "        (df['lower_depth'] >= depth_filters['min_lower'])\n",
    "    )\n",
    "    df = df[depth_mask]\n",
    "\n",
    "    # filter by date\n",
    "    if duration:\n",
    "        df['date'] = df['date'].apply(replace_invalid_dates)\n",
    "        df = df[df['date'].notna()]\n",
    "        df = df[df['date'].between(duration['start_date'], duration['end_date'])]\n",
    "\n",
    "    if df is None or df.empty or len(df) == 0:\n",
    "        logger.warning(f\"No data found for {obs_str}\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"Processed {layer.upper()}: {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "def replace_invalid_dates(date_str):\n",
    "    # Split the date string into components\n",
    "    parts = date_str.split('-')\n",
    "    \n",
    "    # Initialize default values for year, month, and day\n",
    "    year, month, day = '1900', random.randint(1, 12), random.randint(1, 28)\n",
    "    \n",
    "    # Check and validate each part of the date\n",
    "    if len(parts) >= 1 and parts[0].isdigit() and len(parts[0]) == 4:\n",
    "        year = parts[0]\n",
    "    if len(parts) >= 2 and parts[1].isdigit() and 1 <= int(parts[1]) <= 12:\n",
    "        month = parts[1].zfill(2)\n",
    "    if len(parts) >= 3 and parts[2].isdigit() and 1 <= int(parts[2]) <= 31:\n",
    "        day = parts[2].zfill(2)\n",
    "    \n",
    "    # Construct the valid date string\n",
    "    valid_date_str = f\"{year}-{month}-{day}\"\n",
    "    # logger.info(f\"Replaced invalid date: {date_str} -> {valid_date_str}\")\n",
    "    \n",
    "    # Convert to datetime\n",
    "    return pd.to_datetime(valid_date_str, errors='coerce')\n",
    "\n",
    "# Validate configuration\n",
    "country_name, output_path, bounding_box = validate_config(\n",
    "    input_file, layer_prefix, required_layers, required_columns, \n",
    "    output_path, country_name, bounding_box\n",
    ")\n",
    "\n",
    "# Load observations once\n",
    "observations = gpd.read_file(input_file, layer=f\"{layer_prefix}observations\")\n",
    "\n",
    "# Define depth filters\n",
    "depth_filters = {\n",
    "    'max_upper': max_upper_depth,\n",
    "    'max_lower': max_lower_depth,\n",
    "    'min_upper': min_upper_depth,\n",
    "    'min_lower': min_lower_depth\n",
    "}\n",
    "\n",
    "# filter by date\n",
    "duration = {\n",
    "    'start_date': start_date,\n",
    "    'end_date': end_date\n",
    "}\n",
    "\n",
    "logger.info(f\"\"\"\n",
    "/**\n",
    "* Starting WOSIS analysis:\n",
    "* Country: {country_name}\n",
    "* Bounding box: {bounding_box}\n",
    "* Depth range: {min_upper_depth}-{max_upper_depth}cm (upper), {min_lower_depth}-{max_lower_depth}cm (lower)\n",
    "* Duration: {start_date} - {end_date}\n",
    "*/\n",
    "\"\"\")\n",
    "\n",
    "# Process layers\n",
    "processed_dfs = []\n",
    "merge_columns = [col for col in required_columns if col != 'value_avg']\n",
    "\n",
    "for layer in required_layers:\n",
    "    df = process_layer(\n",
    "        input_file, layer, layer_prefix, country_name,\n",
    "        required_columns, bounding_box, depth_filters, observations, duration=duration\n",
    "    )\n",
    "    if df is not None and len(df) > 0:\n",
    "        processed_dfs.append(df)\n",
    "\n",
    "# Merge all processed dataframes\n",
    "if processed_dfs:\n",
    "    master_df = processed_dfs[0]\n",
    "    for df in processed_dfs[1:]:\n",
    "        master_df = master_df.merge(df, on=merge_columns, how='outer')\n",
    "    \n",
    "    # Convert to GeoDataFrame and save\n",
    "    master_df = gpd.GeoDataFrame(master_df)\n",
    "    output_file = os.path.join(output_path, f\"{country_name}_wosis_merged.gpkg\")\n",
    "    master_df.to_file(output_file, driver=\"GPKG\")\n",
    "\n",
    "    # save to csv\n",
    "    master_df.to_csv(output_file.replace(\".gpkg\", \".csv\"), index=False)\n",
    "    \n",
    "    logger.info(f\"Total records in final dataset: {len(master_df)}\")\n",
    "    logger.info(f\"Results saved to: {output_file}\")\n",
    "else:\n",
    "    logger.warning(\"No data found for any layers\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
